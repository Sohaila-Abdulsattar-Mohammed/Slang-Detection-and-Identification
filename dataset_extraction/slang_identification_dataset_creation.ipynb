{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoJtsbGljuhK",
        "outputId": "136b094d-ae9f-469c-f3d9-f13b0528a552"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "print(\"Loading dataset...\")\n",
        "ds = load_dataset(\"MLBtrio/genz-slang-dataset\")\n",
        "\n",
        "# Convert dataset to pandas DataFrame\n",
        "df_hf = ds[\"train\"].to_pandas()\n",
        "\n",
        "# Function to tokenize sentence with punctuation as separate tokens\n",
        "def tokenize_with_punctuation(sentence):\n",
        "    return re.findall(r\"\\w+|[.,!?;+]\", sentence)\n",
        "\n",
        "# Function to generate BIO tags\n",
        "def generate_bio_tags(sentence, slang_term):\n",
        "    lower_sentence = sentence.lower()\n",
        "    lower_slang_term = slang_term.lower()\n",
        "    words = tokenize_with_punctuation(lower_sentence)\n",
        "    slang_words = lower_slang_term.split()\n",
        "    bio_tags = []\n",
        "\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        if words[i:i+len(slang_words)] == slang_words:\n",
        "            bio_tags.append(\"B\")\n",
        "            bio_tags.extend([\"I\"] * (len(slang_words) - 1))\n",
        "            i += len(slang_words)\n",
        "        else:\n",
        "            bio_tags.append(\"O\")\n",
        "            i += 1\n",
        "\n",
        "    return words, bio_tags\n",
        "\n",
        "# Process Hugging Face dataset\n",
        "final_data_hf = []\n",
        "for _, row in df_hf.iterrows():\n",
        "    sentence = row[\"Example\"]  # Use example as the sentence\n",
        "    slang_term = row[\"Slang\"]  # Use slang for tagging\n",
        "    words, bio_tags = generate_bio_tags(sentence, slang_term)\n",
        "    original_words = tokenize_with_punctuation(sentence)  # Retain original casing for output\n",
        "    final_data_hf.append({\n",
        "        \"sentence\": \" \".join(original_words),\n",
        "        \"word_labels\": \",\".join(bio_tags)\n",
        "    })\n",
        "\n",
        "final_df_hf = pd.DataFrame(final_data_hf)\n",
        "\n",
        "# Load the local dataset\n",
        "file_path = \"slang_OpenSub.tsv\"\n",
        "df_local = pd.read_csv(file_path, sep=\"\\t\")\n",
        "\n",
        "# Filter rows with ANNOTATOR_CONFIDENCE >= 2\n",
        "df_local_filtered = df_local[df_local[\"ANNOTATOR_CONFIDENCE\"] >= 2]\n",
        "\n",
        "# Process local dataset\n",
        "final_data_local = []\n",
        "for _, row in df_local_filtered.iterrows():\n",
        "    sentence = row[\"SENTENCE\"]\n",
        "    slang_term = row[\"SLANG_TERM\"]\n",
        "    words, bio_tags = generate_bio_tags(sentence, slang_term)\n",
        "    original_words = tokenize_with_punctuation(sentence)  # Retain original casing for output\n",
        "    final_data_local.append({\n",
        "        \"sentence\": \" \".join(original_words),\n",
        "        \"word_labels\": \",\".join(bio_tags)\n",
        "    })\n",
        "\n",
        "final_df_local = pd.DataFrame(final_data_local)\n",
        "\n",
        "# Combine the two datasets\n",
        "final_combined_df = pd.concat([final_df_hf, final_df_local], ignore_index=True)\n",
        "\n",
        "# Save to a new file\n",
        "output_file = \"bio_tagged_dataset.csv\"\n",
        "final_combined_df.to_csv(output_file, index=False)\n",
        "\n",
        "# Display the final combined DataFrame\n",
        "def display_dataframe_to_user(name: str, dataframe: pd.DataFrame):\n",
        "    print(f\"Displaying DataFrame: {name}\")\n",
        "    print(dataframe.head())\n",
        "\n",
        "display_dataframe_to_user(\"Final Combined BIO Tagged Dataset\", final_combined_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5Re0Mkek_L4",
        "outputId": "8f6efd06-1809-4af6-88ae-710d54db0c4f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "Displaying DataFrame: Final Combined BIO Tagged Dataset\n",
            "                                            sentence  \\\n",
            "0                        Got the job today , big W !   \n",
            "1         I forgot my wallet at home , that s an L .   \n",
            "2  Your tweet got 5 likes and 100 replies calling...   \n",
            "3                             That meme is so dank !   \n",
            "4  That phrase is so cheugy , no one says that an...   \n",
            "\n",
            "                       word_labels  \n",
            "0                  O,O,O,O,O,O,B,O  \n",
            "1          O,O,O,O,O,O,O,O,O,O,B,O  \n",
            "2  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O  \n",
            "3                      O,O,O,O,B,O  \n",
            "4          O,O,O,O,B,O,O,O,O,O,O,O  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tag these sentences mannually"
      ],
      "metadata": {
        "id": "tZDfzmRPoInu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count rows with all \"O\" in the word_labels column and get line numbers\n",
        "all_o_rows = final_combined_df[final_combined_df[\"word_labels\"].apply(lambda x: set(x.split(\",\")) == {\"O\"})]\n",
        "line_numbers = [index + 2 for index in all_o_rows.index.tolist()]\n",
        "print(f\"Number of rows with all 'O': {len(all_o_rows)}\")\n",
        "print(f\"Line numbers of rows with all 'O': {line_numbers}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tieN8j7rmc10",
        "outputId": "8cd103aa-7853-4105-9001-9c77b5ef0361"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows with all 'O': 228\n",
            "Line numbers of rows with all 'O': [4, 10, 15, 22, 23, 25, 27, 28, 32, 41, 42, 47, 50, 52, 54, 62, 64, 76, 92, 93, 110, 119, 120, 137, 142, 146, 156, 176, 190, 195, 198, 214, 215, 216, 222, 236, 237, 238, 239, 240, 241, 242, 243, 244, 280, 293, 294, 369, 386, 413, 419, 420, 451, 459, 493, 496, 538, 568, 607, 640, 738, 746, 792, 793, 805, 813, 835, 857, 877, 887, 891, 896, 986, 992, 994, 999, 1007, 1108, 1126, 1142, 1166, 1328, 1413, 1431, 1439, 1442, 1461, 1483, 1485, 1495, 1529, 1574, 1586, 1595, 1629, 1630, 1631, 1645, 1652, 1653, 1674, 1681, 1712, 1714, 1724, 1775, 1788, 1833, 1848, 1891, 1893, 1972, 2012, 2051, 2075, 2076, 2109, 2135, 2194, 2245, 2257, 2270, 2274, 2291, 2368, 2369, 2391, 2417, 2442, 2460, 2497, 2506, 2536, 2700, 2708, 2714, 2716, 2726, 2729, 2731, 2748, 2756, 2757, 2758, 2763, 2781, 2803, 2804, 2835, 2861, 2929, 2932, 2934, 2942, 2950, 2953, 2958, 2969, 2992, 3012, 3052, 3092, 3126, 3128, 3130, 3142, 3143, 3163, 3164, 3206, 3215, 3236, 3240, 3251, 3294, 3309, 3318, 3340, 3341, 3346, 3382, 3385, 3400, 3410, 3412, 3414, 3453, 3479, 3485, 3486, 3491, 3493, 3513, 3516, 3524, 3526, 3573, 3614, 3617, 3641, 3645, 3651, 3669, 3694, 3741, 3756, 3761, 3763, 3769, 3820, 3831, 3852, 3871, 3877, 3888, 3903, 3913, 3929, 3935, 3941, 3947, 3953, 3965, 3975, 4001, 4015, 4019, 4031]\n"
          ]
        }
      ]
    }
  ]
}